Chapter 2: Intelligent Agents:

Keywords:
1. Rational Agents (perceive environment, act using actuators)
2. Sensor
3. Actuator
4. Percept
5. Percept sequence
6. Agent function (abstract mathematical characterization)
7. Agent program (actual implementation)
8. Priori (the environment is already completely known.)
9. Omniscient agents
10. Task Environments
11. Software agents (softbots)
12. Fully observable
13. Partially observable
14. Single agent
15. Multiagent
16. Episodic
17. Sequential
18. Static environment
19. Dynamic environment
20. Discrete state
21. Continuous state
22. Known environment
23. Unknown environment
24. Environment class
25. Environment generator

- 2.2 Good Behavior: The Concept of Rationality
  • We want a rational agent to do the right thing. In previous pages, the
  rational agent would fill out the every entry in the agent function correctly.
  • What are the consequences of an agent's behavior?
  • The notion of desirability is captured through performance measure
    - Evaluates any given sequence of environment states.
    - Choosing the right reward is difficult, and depends on the given situation
  • Rationality at any given time:
    - Agent's prior experience with the environment.
    - What the agent's potential actions are.
    - The agent's percept sequence to date.
    - The performance measure that determines success.
  • Rational Agent:
    - For each possible percept sequence, a rational agent should select an action
     that is expected to maximize its performance measure, given the evidence
     provided by the percept sequence and whatever built-in knowledge the agent has.
  • Difference between a rational and omniscient agent:
    - Omniscient agents know the actual outcome of its actions and can act accordingly.
      • Rationality depends on information known to date, can't see future outcomes.
    - Rationality maximizes expected performance, while perfection maximizes actual performance.
  • Information Gathering: doing actions in order to modify future percepts, or through exploration.
  • We want the rational agent to do both information gathering and also learn from its perceptions.
    - In the case of a priori, the agent doesn't need to gather info or learn, it
    just needs to act accordingly.
  • An agent lacks autonomy if an agent relies on the prior knowledge of the designer,
  instead of its own percepts.
    - When an agent has little experience, then acting randomly makes more sense than
    acting upon its known percepts (or designer gives some assistance.)
    - The incorporation of learning allows a rational agent to succeed in a variety of environments.

- 2.3 The Nature of Environments:
  • Task environment: the grouping of performance measure, environment, and agent's actuators/sensors.
    - This is known as the PEAS acronym.
    - This should be the first step in designing an agent, and want to be as detailed as possible.
  • Fully observable: the agent can perceive the environment fully through the usage of its sensors.
    - Technically, an agent is fully observable if the sensors give it relevant to the choice of action
    at any given point in time.
    - This is useful as the agent won't need to track any internal state to keep track of environment.
  • Partially observable: some information from the sensors is missing, or the state is incomplete.
  • Environments can be either deterministic or stochastic.
    - If the next state in the environment is completely determined by the current state and the action
    is executed by the agent. (environment is uncertain if not fully observable/deterministic.)
  • In an episodic task environment, the agent's experience is divided into atomic episodes.
    - The next episode does not depend on the actions taken in previous episodes.
    - Ex. The previous part was defunct in an assembly line, but that doesn't influence the current part.
  • Sequential task environment: previous decisions could impact future decisions.
  • Static environment: environment doesn't change while the agent is deliberating; else dynamic
    - Easier to deal with as the agent does not need to gathering more information while deciding on an action.
    - Semidynamic environment is where the environment doesn't change, but the agent's performance does.
  • Discrete environment: applies to the state of the environment, in the way that time is handled.
  • It's possible for an environment to be both known and partially observable.
  • Overall, the hardest case is partially observable, multiagent, stochastic, sequential,
  dynamic, continuous, and unknown.
  • Environment class: experiments conducted using multiple environments
  • Environment generator: used to test each environment class against particular environments

- 2.4 The Structure of Agents:
  • Agent program: the job of AI and implements the agent function (mapping of percepts to actions.)
    - Agent = architecture + program.
  • Condition-action rule: if ... then ...
  • Four basic kinds of agent programs:
    1. Simple reflex agents:
      - Simplest kind, select actions of the basis of the current percept.
      - Limited intelligence, correct decisions may only be possible with fully
      observable environments.
    - Infinite loops are sometimes unavoidable in this implementation.
      • Although escaping is possible if agent is able to randomize its actions, and
      a randomized single reflex agent may outperform a deterministic single reflex agent.
      • Randomization is usually not rational in single agent environments.
    2. Model-based reflex agents:
      • Internal state to handle partial observable environments. Keep track of what the agent can't perceive.
        - To implement, we need to understand how the environment evolves independent of the agent,
        and how the agent's actions affect the environment.
      • We use internal states to explain how the environment works, effectively making a model of the world.
      • Use state information, world modeling, and action affects to send information to what action to
      carry out now, which takes into account the condition-action rules, and then sends to the actuators.
    3. Goal-based agents:
      • Sometimes knowing the current state of the environment is not enough: we need a goal in mind, in addition
      to the current state information (i.e. going to the passenger's destination.)
      • This agent can combine knowledge in the model-based reflex agent to choose actions to achieve the defined goal.
      • The AI subfields, search and planning, are devoted to finding action sequences that achieve the agent's goals.
      • Sacrificing efficiency (model-based go directly from percepts to actions) for flexibility.
        - This is because the information gathered is explicitly defined, and mutable.
        - i.e. It starts raining and the taxi can update its knowledge of how effectively brakes will operate.
    4. Utility-based agents:
      • Goal based agents are always enough to generate high quality behavior in most environments.
      • Utility: general performance measure that compares different world states according to exactly how
      happy they would make the agent feel.
      • Utility function: an internalization of the performance measure.
        - The performance measure assigns a score to any given sequence of environment states.
          • Used to distinguish between good/bad/optimal decisions.
        - An agent that chooses to maximize its utility will be rational.
        - This is useful when tradeoffs need to occur between two defined goals (speed vs. safety)
        - Utility provides a weighted measure: the probability of success vs. importance of goals.
  • Learning has the advantage where you can place an agent in an unknown environment
  and become more competent than its initial knowledge alone might allow.
  • Conceptual component of a learning agent:
    - Learning element: responsible for making improvements.
      • Uses feedback from a critic to determine how the performance element
      should be modified for future success.
    - Performance element: responsible for selecting external actions.
      • Takes percepts and decides on actions.
      • Both the learning and performance element successes depend on each other.
    - Critic: determines how the agent is doing
    - Problem generator: responsible for suggesting actions that will lead to
    new and informative experiences.
      • The performance element will always choose actions that are the best, but
      this isn't always the best course of action and the agent may discover better
      actions in the long-run for short-term suboptimal actions.
  • Performance standard:
    - This distinguishes part of the incoming percept as a reward/penalty that
    provides direct feedback on the quality of the agent's behavior.
  • Three basic representations of states and transitions between them:
    1. Atomic: each state of the world is indivisible, meaning it has no internal state
      - Search and game-playing algorithm
      - Hidden Markov models
      - Markov decision processes
    2. Factored representation: split up each state into a fixed set of variables/attributes,
    each of which have an associated value.
      - Two factored states can share some attributes, whereas atomic representations
      are essential two different black boxes.
      - Can represent uncertainty
      - Areas of AI based on factored representations:
        • Constrain satisfaction algorithms
        • Propositional logic
        • Planning
        • Bayesian networks
        • Machine learning algorithms
    3. Structured representation: various objects and varying relationships
    are described explicitly.
      - Relational databases
      - First-order logic
      - First-order probability models
      - Knowledge-based learning
      - Natural language understanding 
